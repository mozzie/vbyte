\chapter{Introduction}
Enormous datasets are a common case in today's applications. Compressing the datasets is beneficial, because they 
naturally decrease memory requirements but also are faster when compressed data is read from disk \citep{Zob95}.

One of the leading methods of data compression is variable-length coding \citep{Sal99}, where frequent sequences of data
are represented with shorter codewords. Because the sequences of data have different lenghts when compressed, it is 
not trivial to determine the exact location of a certain element. If this is required, the usual data compression algorithms are
inefficient. Fortunately this is not a requirement compression algorithms usually need to fulfill. 

However, random access of compressed data is needed in compressed data structures. In most compression methods, the only way 
to this is to decompress data from the beginning. An integer compressing method with fast random access is explained later and compared
existing state-of-the-art methods.

\chapter{Variable-byte encoding}

Variable-byte (VB) encoding \citep{Wil99} is a method of compressing integers via omitting leading zero bits. In normal data sets, it loses in 
compression performance to generic algorithms like Huffman encoding or Lempel-Ziv encoding, but in comparison it's faster to decode \citep{Bri09} 
and sometimes preferred due to it's simplicity. allows constant time random access. This is later explained in (TODO: link to later chapter). 

A good data set for VB encoding is a list of mostly small numbers with a need to support larger ones. A search engine may use an inverted index of 
words in documents. For each word, a list of document IDs where the word appears is stored. It may also store locations of the word in document for 
advanced search purposes. Usually these lists are preprocessed and stored as an inverted index or gaps, storing the difference to previous number instead of the actual 
number \citep{Man08}. Common words have a lot of entries in these lists, but because of gap storing the numbers are small. In contrast, rare words 
have only a few entries but the numbers stored are larger. These lists are excellent data sets for VB encoding. 

VB encoding splits each integer into blocks of $b$ bits and adds a continuation bit to the block to form chunks of length $1+b$. The extra bit is set to 1 only
on the block containing the least significant bits of the integer. This information is used in decoding to signal if next chunk continues the current 
integer. For example, let's assume $b = 4$ and let $n = 42$ be a 16 bit unsigned integer. The standard 8-bit representation of $n$ is 
\texttt{00000000 00101000}. When split to blocks of $b$ bits, it becomes \texttt{0000 0000 0010 1000}. Empty blocks are omitted and continuation bits 
are added to the remaining blocks. The result is \texttt{00010 11000}, which is the compressed data.

\medskip
\noindent\begin{minipage}{.5\textwidth}
\begin{algorithmic}
\Function{VBEncodeNumber}{$n$}
\State $bytes\gets $ list
\While{true}
\State $bytes$.prepend($n \bmod 128$)
\If{$n < 128$} \State break \EndIf
\State $n\gets n$ div $128$
\EndWhile
\State $bytes$.last() += $128$
\State \textbf{return} bytes
\EndFunction
\captionof{figure}{VByte encoding} \label{vbyte_enc}
\end{algorithmic}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{algorithmic}
\Function{VBEncode}{$numbers$}
\State $bytestream\gets $ list
\ForEach {$n \in numbers$}
\State $bytes \gets$ VBEncodeNumber($n$)
\State $bytestream$.extend($bytes$)
\EndFor
\State \textbf{return} bytestream
\EndFunction
\end{algorithmic}
\end{minipage}
\medskip 

Decoding is essentially just reversing the encoding steps: chunks are read until a chunk with 1 as continuation bit is found. Continuation bits 
are removed from all the chunks and the blocks are concatenated to form the original number. As in the previous example, $b = 4$, encoded message 
is \texttt{00010 11000} and $n = 0$. Block from the first chunk is extracted and added to $n$, making $n = $\texttt{10}. A bitwise shift to left 
equal to $b$ is applied to $n$, changing $n = $\texttt{100000}. Then the block is extracted from the next chunk. This block is added  $n$, making 
$n = 42$. Because the previous continuous bit was 1, decoding this number has finished. An example implementation of encode and decode with block 
length of 7 is shown in Figure~\ref{vbyte_enc} and Figure~\ref{vbyte_dec}.

\begin{figure}[ht]
\begin{algorithmic}
\Function{VBDecode}{$bytestream$}
\State $numbers\gets $ list
\State $n\gets 0$
\ForEach {$b \in bytestream$}
\If{$b < 128$}
\State $n\gets 128\times n $ + $b$
\Else
\State $n\gets 128\times n $ + $b$ - $128$
\State $numbers$.append($n$)
\State $n\gets 0$
\EndIf
\EndFor
\State \textbf{return} numbers
\EndFunction
\end{algorithmic}
\caption{VByte decoding} \label{vbyte_dec}
\end{figure}

Small lengths of $c$ can yield better compression rate at the cost of more bit manipulation, while longer chunks need less bit manipulation and 
offer less effective compression. Generally block length of 7 has been used because it gives a good average and handling chunks as bytes is 
convenient \citep{Man08}.

VB encoding is a well known compression algorithm. It's origins date back to 1980's and the famous MIDI music file format. It stored some of the numbers
in a "variable-length quantity" form, which was a 7-bit block VB structure \citep{Mid96}. Similar data types have existed for example in Apache Lucene 
(as vInt) and IBM DB2 database (as Variable Byte). Later, VB encoding was found efficient in compressing integer lists. It was first experimented in 
compressing inverted index lists of word locations in documents \citep{Sch02}. That yielded excellent records, and since then many different approaches 
have been introduced. 

Modern studies have looked into machine code for VB and applied SIMD (Single instruction, multiple data) architecture to VB \citep{Lem18,Pla15}. The bit 
operations in VB are simple and therefore modifying the code to use SIMD instructions is straightforward and the speed improvements are significant. 

TODO: might need 1-2 more chapters


\chapter{Directly addressable codes?}

Ability to handle large amounts of data fast is one of the key challenges in the field of search engines. Compressed data structures are applied to fit the 
data into cache, memory or even hard drive. Direct access to any element is one of the usual requirements in compressed data strutures. It is not natively
possible to decode the i-th element in variable-byte compression algorithms, because the position of the element depends on the preceding data. Direct access 
is achievable with supporting data structures. An obvious solution is to store each element's location, but that adds a very large overhead which removes the 
benefit of compression, but for some compression techniques it adds a very large overhead and thus is not applicable.

Rank and select are two succinct data structure operations which operate on a bit array B. $Rank_1$(B, i) gives the sum of 1 bits between B[0] and B[i] and 
$select_1$(b, i) gives the index of i-th 1 bit in B. Both operations work in constant time (citation?) and they require only a few percents of extra 
space over the bit array B. See Table~\ref{supportsize} for $rank$ and $select$ size requirement approximations on different sizes of bit arrays. 

Locations of 1 bits in B should represent locations of the items in the encoded data. For most compression algorithms, this requires B to be created in addition 
to the existing data and the length of B usually has to be close to the length of the data. VB encoding has several advantages with search and rank: it's data 
is compressed in blocks of equal length which significantly shrinks the length of B. Also the bit array C formed from the continuation bits already stores the 
locations of items. In this case, $rank_1$(C, i) would give the sum of end bytes up to i-th index and $select_1$(C, i) would give the location of the ending 
byte of i-th compressed element.

\begin{table}
\centering
\caption{Size of support structure with SDSL-lite library}
\begin{tabular}{l||c c c c c c} 
Structure & TODO & get & multiple & size & calculations & here\\ 
\hline \hline 
$rank$ & 34.97 & 49 & 53.04 & 52.18 & 53.08 & 76.21\\
$select$ & 33.57 & 32.47 & 42.96 & 43.11 & 46.15 & 65.14\\
\hline
%
\end{tabular}
\label{supportsize}
\end{table}

\section{DAC via rank}
Directly addressable codes in VB was first introduced by \citep{Bri09} in 2009. Their solution was to divide chunks in encoded bytes to separate arrays $A_1$ to 
$A_n$ and then use $rank_1$ to get direct access. For example, if an element needed 3 chunks when encoded, the least significant bits would go to $A_1$, second 
least significant bits to $A_2$ and the most significant bits would go to $A_3$. When fetching i-th element, getting the first chunk is just fetching $A_1$[i]. 
If the continuation bit is set at 0 (meaning there are more chunks to this element), getting the correct index for $A_2$ is calculated with $rank_1$($A_1$, i). 
Small values benefit from this reordering, because the first chunk does not need any calculation. This method causes a small overhead on the data, because each 
continuation bit array needs a support data structure for rank. 

do rank/select algorithms + data structures need to be explained better? (or just referenced to)

<introduce Bri09 better?>

<example of Bri09>

- explain how random access is good ?

\chapter{DAC with select query}

Using $select_1$ on the continuation bit array to achieve direct access is more intuitive than using $rank_1$. The element locations are already marked with 1's 
in $B$ and a single $select_1$ query gets the desired starting point, while the forementioned version \citep{Bri09} used one $rank$ query for each chunk beyond 
the first. Minimizing the amount of $select$ and $rank$ queries is important. They run in constant time but their impact is huge, because rest of the VB decoding 
consists of a few bit operations. 

To use $select_1$ with VB, continuation bits need to be separated from chunks to their own bit array and a $select_1$ structure built over it. $Select$(i) returns the
location of the end byte of i-th element. Therefore the start of j-th element in the block array is at block $b_s$ = $select_1$(j-1) + 1. To simplify calculations even further,
block sizes of 2, 4 and 8 were used to avoid splitting of blocks between bytes.

The blocks should be stored in an order that, depending on the endianness of the system, when reading an unsigned integer from the block array, the blocks are already 
in correct order. This way two bit shift operations are enough to extract the desired value.  Assume original element size is 32 bits, block length is b and requested
element is at position x in the block array and it's compressed within k blocks. Starting byte s of element e is x * b div 8 where div is the integer division. Offset o
is the remainder of previous division, o = x*b mod 8. 32-bit value e is read from memory from s. Then we bit shift left 32-b*k-o to remove trailing bits and bit shift right
32-b*k to re-align bits. 

Simple way to calculate block length k of i-th element is to get $select_1$(i+1) and subtract previously calculated start block index from it. This however causes a second 
$select_1$ query, which costs a lot timewise. A much faster option is to iterate forward in bit array until next 1 is found. An even faster option is to read an integer from 
the bit array, fix it's offset and count trailing zeros. Traveling the bit array is recommended, since it is very simple to implement and is not noticably slower. See 
Table~\ref{endblockcall} for detailed times.

\begin{table}
\centering
\caption{Time to calculate end blocks}
\begin{tabular}{l||c c c c c c} 
Version & Average time over 10Mall \\ 
\hline \hline 
select(i+1) & 34.97 \\
iterate bit array & 34.97 \\
bit shift magic & 34.97 \\
\hline
%
\end{tabular}
\label{endblockcall}
\end{table}

(maybe to future work) Storing bytes in this fashion may have benefits when reading subsequent (fast next() and previous()?) elements. With some modifications, SIMD instructions can possibly be applied.



TODO: check if rank query takes less when done on single byte array instead of multiples

 - get actual results of rank vs select support size
 - take advantage of how data is stored

 - emphasis on size!

     RANK seems a lot faster, check algorithms!!!
TODO: try bitvector SD performance + size (select)

TODO: look into select implementation, may need optimizing

\chapter{Experimental results}

The following experiments are run on <at the moment my work Lenovo Thinkpad T480s, TODO run on a server>.

Five different kinds of VB decoding algorithms were used. Bris-implementations are from \citep{Bri09}. Rank and select functions are from
 \citep{gbmp2014sea}.

\begin{itemize}
  \item Bris4v5 - Bris implementation with block size 4, using rank support v5
  \item Bris8 - Bris implementation with block size 8, using rank support v
  \item Bris8v5 - Bris implementation with block size 8, using rank support v5
  \item Our4 - Our implementation with block size 4, using select support mcl
  \item Our8 - Our implementation with block size 8, using select support mcl
\end{itemize}

Eight kinds of datasets were used. For each number in the dataset, $p$ was randomly selected from $P$ and then number was randomly selected from
range [0,$2^p$].

\begin{itemize}
  \item all5M - 5 million numbers, $P$ = {7,8,15,16,23,24,30,31}
  \item all50M - 50 million numbers, $P$ = {7,8,15,16,23,24,30,31}
  \item byte5M - 5 million numbers, $P$ = {7,7,7,8,8,8,16,31}
  \item byte50M - 50 million numbers, $P$ = {7,7,7,8,8,8,16,31}
  \item small5M - 5 million numbers, $P$ = {3,4,5,6,7,8,16,31}
  \item small50M - 50 million numbers, $P$ = {3,4,5,6,7,8,16,31}
  \item vsmall5M - 5 million numbers, $P$ = {2,2,3,3,3,4,4,15}
  \item vsmall50M - 50 million numbers, $P$ = {2,2,3,3,3,4,4,15}
\end{itemize}

All algorithms save the data to memory and randomize one million index numbers to an array. The time taken is calculated from looping through the 
index array and VB decoding the number in the index. The times shown in Table~\ref{table:results1} are an average of 100 such runs.

\begin{table}
\centering
\caption{Results in milliseconds, smaller is better.\label{table:results1}}
\begin{tabular}{l||c c c c c c c c} 
Experiment & all5M & all50M & byte5M & byte50M & small5M & small50M & vsmall5M & vsmall50M\\ 
\hline \hline 
Bris4v5 & 348.41 & 768.83 & 345.16 & 772.84 & 344.54 & 768.45 & 346.86 & 771.61 \\
Our4    & 127.92 & 263.22 & 127.44 & 263.5  & 127.9  & 262.67 & 127.88 & 262.58 \\
Bris8   & \textbf{97.75}  & 308.17 & \textbf{98.47}  & 309.6  & \textbf{98.14}  & 307.81 & \textbf{97.99}  & 308.09 \\
Bris8v5 & 127.83 & 287.46 & 127.7  & 288.63 & 128.14 & 291.32 & 127.99 & 287.84 \\
Our8    & 103.85 & \textbf{230.16} & 103.92 & \textbf{230.14} & 103.86 & \textbf{230.02} & 103.95 & \textbf{230.6} \\

\hline
%
\end{tabular}
\end{table}

 - use array instead of vector<uint8 t> for bris?
- comparison to basic implementation + Bri09
  - compare with b=2,4,8

 - do we need to support search()?
 - SIMD?


\chapter{Conclusion}
 - here

\chapter{Future work}
 - something to improve / research?

\chapter{Conclusions\label{chapter:conclusions}}

It is good to conclude with a summary of findings. You can also use separate chapter for discussion and future work. These details you can negotiate with your supervisor.
