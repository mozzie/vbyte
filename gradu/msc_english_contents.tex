\chapter{Introduction}
Enormous datasets are a common case in today's applications. Compressing the datasets is beneficial, because they 
naturally decrease memory requirements but also are faster when compressed data is read from disk \citep{Zob95}.

One of the leading methods of data compression is variable-length coding \citep{Sal99}, where frequent sequences of data
are represented with shorter codewords. Because the sequences of data have different lengths when compressed, it is 
not trivial to determine the exact location of a certain element in the compressed data. If this is required, the usual 
data compression algorithms are inefficient. Fortunately this is not a requirement compression algorithms usually need to fulfill. However, 
random access of compressed data is useful in compressed data structures. It saves storage space, bandwith and increases the likelihood
of data already being in cache \citep{Sch02}. In most compression methods, the only way to do this is to decompress data from the beginning. 

A variable-byte encoding based integer compression method with fast random access was first introduced by \citep{Bri09}. They used clever block 
reorganizing and $rank$ data structure to achieve random access. Their solution is currently the only published solution for the problem and it 
has been widely adopted.

An alternative solution with $select$ data structure is shown later. (TODO: refer to results)

--contributions?




\chapter{Variable-byte encoding}

Variable-byte (VB) encoding \citep{Wil99} is a method for compressing integers via omitting leading zero bits that would be present in a longer fixed 
width word. In normal data sets, VB encoding loses in compression performance to generic algorithms like Huffman encoding or Lempel-Ziv encoding, but 
is generally faster to decode \citep{Bri09} and sometimes preferred due to its simplicity. As later shown in Chapter~\ref{chapter:DAC}, constant time 
random access is possible with VB encoding.

A good data set for VB encoding is a list of mostly small numbers with a need to support larger ones. A search engine may use an inverted index of 
words in documents. For each word, a list of document IDs where the word appears is stored. It may also store locations of the word in document for 
advanced search purposes. Usually these lists are preprocessed and stored as an inverted index or gaps, storing the difference to previous number 
instead of the actual number \citep{Man08}. Common words have a lot of entries in these lists, but because of gap storing the numbers are small. 
In contrast, rare words have only a few entries but the numbers stored are larger. These lists are excellent data sets for VB encoding. 

Variable-byte encoding originates from and is used in MIDI music file \citep{Mid96} and several applications have a similar implementation of VB. Apache 
Lucene has vInt datatype. Wireless Application Protocol has a variable length unsigned integer uintvar, Google Protocol Buffers has a Base 128 Varint \citep{GooPB},
 Microsoft .Net framework offers "7BitEncodedInt" in BinaryReader and BinaryWriter classes and IBM DB2 has a variable byte  \citep{Bha09}.

Elias Delta and Gamma codes \citep{Eli75} are popular encoding methods for forementioned data sets. They assign short bit array codes to small numbers, 
which allows them to outperform VB encoding on small number datasets \Citep{Wil99}. Elias Delta and Gamma codes can't support fast random access efficiently because
of their changing code length.

VB encoding splits each integer into blocks of $b$ bits and adds a continuation bit to the block to form chunks of length $1+b$. The extra bit is set to 1 only
on the block containing the least significant bits of the integer. This information is used in decoding to signal if next chunk continues the current 
integer. For example, let's assume $b = 4$ and let $n = 42$ be a 16 bit unsigned integer. The standard 8-bit representation of $n$ is 
\texttt{00000000 00101000}. When split to blocks of $b$ bits, it becomes \texttt{0000 0000 0010 1000}. Empty blocks are omitted and continuation bits 
are added to the remaining blocks. The result is \texttt{00010 11000}, which is the compressed data.
\begin{figure}[ht]
\begin{algorithmic}
\Function{VBEncodeNumber}{$n$}
\State $bytes\gets $ list
\While{true}
\State \Call{prepend}{bytes,$n \bmod 128$}
\If{$n < 128$} \State break \EndIf
\State $n\gets n$ div $128$
\EndWhile
\State $bytes$[\Call{len}{$bytes$}-1] += $128$
\State \textbf{return} bytes
\EndFunction

\Function{VBEncode}{$numbers$}
\State $bytestream\gets $ list
\ForEach {$n \in numbers$}
\State $bytes \gets$ VBEncodeNumber($n$)
\State $bytestream$.extend($bytes$)
\EndFor
\State \textbf{return} bytestream
\EndFunction

\end{algorithmic}
\captionof{figure}{VByte encoding} \label{vbyte_enc}
\end{figure}

Decoding is essentially just reversing the encoding steps: chunks are read until a chunk with 1 as continuation bit is found. Continuation bits 
are removed from all the chunks and the blocks are concatenated to form the original number. As in the previous example, $b = 4$, encoded message 
is \texttt{00010 11000} and $n = 0$. Block from the first chunk is extracted and added to $n$, making $n = $\texttt{10}. A bitwise shift to left 
equal to $b$ is applied to $n$, changing $n = $\texttt{100000}. Then the block is extracted from the next chunk. This block is added  $n$, making 
$n = 42$. Because the previous continuous bit was 1, decoding this number has finished. An example implementation of encode and decode with block 
length of 7 is shown in Figure~\ref{vbyte_enc} and Figure~\ref{vbyte_dec}.

\begin{figure}[ht]
\begin{algorithmic}
\Function{VBDecode}{$bytestream$}
\State $numbers\gets $ list
\State $n\gets 0$
\ForEach {$b \in bytestream$}
\If{$b < 128$}
\State $n\gets 128\times n $ + $b$
\Else
\State $n\gets 128\times n $ + $b$ - $128$
\State $numbers$.append($n$)
\State $n\gets 0$
\EndIf
\EndFor
\State \textbf{return} numbers
\EndFunction
\end{algorithmic}
\caption{VByte decoding} \label{vbyte_dec}
\end{figure}

Small lengths of $c$ can yield better compression rate at the cost of more bit manipulation, while longer chunks need less bit manipulation and 
offer less effective compression. Generally block length of 7 has been used because it gives a good average and handling chunks as bytes is 
convenient \citep{Man08}.

VB encoding is a well known compression algorithm. It's origins date back to 1980's and the famous MIDI music file format. It stored some of the numbers
in a "variable-length quantity" form, which was a 7-bit block VB structure \citep{Mid96}. Similar data types have existed for example in Apache Lucene 
(as vInt) and IBM DB2 database (as Variable Byte). Later, VB encoding was found efficient in compressing integer lists. It was first experimented in 
compressing inverted index lists of word locations in documents \citep{Sch02}. That yielded excellent records, and since then many different approaches 
have been introduced. 

Modern studies have looked into machine code for VB and applied SIMD (Single instruction, multiple data) architecture to VB \citep{Lem18,Pla15}. The bit 
operations in VB are simple and therefore modifying the code to use SIMD instructions is straightforward and the speed improvements are significant. 

TODO: might need 1-2 more chapters


\chapter{Directly addressable codes} \label{chapter:DAC}

Ability to handle large amounts of data fast is one of the key challenges in the field of search engines. Compressed data structures are applied to fit the 
data into cache, memory or even hard drive. Direct access to any element is one of the usual requirements in compressed data strutures. It is not natively
possible to decode the i-th element in variable-byte compression algorithms, because the position of the element depends on the length of the preceding 
compressed data. 

Direct access is achievable with supporting data structures. An obvious solution is to store the location of each element, but it adds a very large overhead 
which removes the benefit of compression.

\section{Rank and Select}
Rank and select are two succinct data structure operations which operate on a bit array B. $Rank_1$(B, i) gives the sum of 1 bits between B[0] and B[i] and 
$select_1$(b, i) gives the index of i-th 1 bit in B. Both operations work in constant time (citation?). Locations of 1 bits in B should represent locations 
of the items in the encoded data. For most compression algorithms, this requires B to be created in addition to the existing data and the length of B usually 
has to be close to the length of the data. VB encoding has several advantages with search and rank: it's data is compressed in blocks of equal length which 
significantly shrinks the length of B. Also the bit array C formed from the continuation bits already stores the locations of items. In this case, $rank_1$(C, i) 
would give the sum of end bytes up to i-th index and $select_1$(C, i) would give the location of the ending byte of i-th compressed element.

\section{Rank and Select implementation}

The rank and select implementations used in this article are C++ libraries from \citep{gbmp2014sea}. The library has several implementations of both rank and 
select as well as an implementaion of a bit array. Table~\ref{table:supportsize} has $rank$ and $select$ size requirements of implementations used in this experiment.
Both rank implementations have a constant space requirement, while select's needed size depends on the number of 1's in the data.

For every 512th bit in the array, rank stores the absolute value of 1's preceding that bit in a superblock. For every 64th bit in a superblock, a relative 
count is stored. With this setup, an rank can be instantly calculated for every 64th bit. To get rank(i), superblock[i/512] and it's relative value (i\%512)/64 
are fetched. Then 1 bits from (i-i\%64) to i are counted and added to get the correct sum. For Rankv5, superblock size is 2048 and relative count is stored every
384th bit, which offers a tradeoff between performance and memory usage.

Select data structure works similarly to rank. Index location of every 4096th set bit is stored in the superblock and location of every 64th set bit is stored 
relative to the superblock. 


\begin{table}
\centering
\caption{Memory requirements of SDSL rank and select data structures\label{table:supportsize}}
\begin{tabular}{l||c} 
Structure & Extra size taken\\ 
\hline \hline 
Rank   & \text{25\% of bit array} \\
Rankv5 & \text{6.25\% of bit array}\\
Select & \text{~8.3\% of bit array (on average)}\\
\hline
\end{tabular}
\end{table}



\section{DAC via rank}
Directly addressable codes in VB was first introduced by \citep{Bri09} in 2009. Their solution was to divide chunks in encoded bytes to separate arrays $A_1$ to 
$A_n$ and then use $rank_0$ to get direct access. If i is the requested index, then $rank_0$($A_k$, i) returns the number of elements preceding i that have 
continuation bit set at 0. In other words, the number of elements that continue to $A_{k+1}$. 

For example, if an element needed 3 chunks when encoded, the least significant bits would go to $A_1$, second least significant bits to $A_2$ and the most significant 
bits would go to $A_3$. When fetching i-th element, getting the first chunk is just fetching $A_1$[i]. If the continuation bit is set at 0 (meaning there are more chunks 
to this element), getting the correct index for $A_2$ is calculated with $rank_0$($A_1$, i). A pseudo code example of DAC with $rank$ is shown in Figure~\ref{bris_pseudo} 
with block length of 8. (TODO: example needed?)


\begin{figure}[ht]
\begin{algorithmic}
\State $i \gets $ wanted index
\State $A \gets $ block arrays
\State $B \gets $ continuation bit arrays
\State $level \gets 0$
\State $number \gets 0$
\While {$B[level][index] = 0$}
\State $block \gets A[level][index]$
\State $number \gets number \mathbin{\ll} 8$
\State $number \gets number \mathbin{|} block$
\State $index \gets \Call{Rank}{B[level], index}$
\State $level \gets level + 1$
\EndWhile
\State $block \gets A[level][index]$
\State $number \gets number \mathbin{\ll} 8$
\State $number \gets number \mathbin{|} block$
\end{algorithmic}
\caption{Example pseudo code of DAC with rank by Brisaboa et al.} \label{bris_pseudo}
\end{figure}


\chapter{DAC with select query}

Using $select_1$ on the continuation bit array to achieve direct access is more intuitive than using $rank_1$. The element locations are already marked with 1's 
in $B$ and a single $select_1$ query gets the desired starting point, while the forementioned version \citep{Bri09} used one $rank$ query for each chunk beyond 
the first. Minimizing the amount of $select$ and $rank$ queries is important. They run in constant time but their impact is huge, because rest of the VB decoding 
consists of a few bit operations. 

To use $select_1$ with VB, continuation bits need to be separated from chunks to their own bit array and a $select_1$ structure built over it. $Select$(i) returns the
location of the end byte of i-th element. Therefore the start of j-th element in the block array is at block $b_s$ = $select_1$(j-1) + 1. To simplify calculations even further,
block sizes of 4 and 8 were used to avoid splitting of blocks between bytes.

Unlike the standard VB encoding, the continuation bits are not in the middle of the data blocks. The compressed number can be read from the memory block and concatenation of blocks
is not needed. The data in blocks is be written into memory as it is, so that when reading a large unsigned integer from the block byte array, the bits are already in correct order. 

If the original element size is 32 bits and the data was compressed to k blocks of length b. The starting byte s of the element is $select$(i) * b div 8 where div is the integer division. 
Offset o is the remainder of previous division, o = x*b mod 8. The 32-bit value is read from memory from byte location s. Then bit shift left for 32-b*k-o is applied to remove trailing 
bits and bit shift right 32-b*k to re-align bits. If block size is one byte, offset calculation is not needed and thus the process is slightly faster.

The intuitive way to calculate block length k of i-th element is from $select_1$(i+1) and subtract previously calculated start block index from it. This however causes a second 
$select_1$ query, which is costly. A much faster option is to iterate forward in bit array until the next 1. Alternatively, the block length can be calculated by reading an integer 
from the bit array, aligning it's offset and counting the trailing zeros. Figure~\ref{select_pseudo} contains an example of VB encoding with DAC with $select$ and block size 8. 
Different block sizes need extra calculation to get the block location from the byte array. In the example, $CalculateLength$ returns the length of the number in blocks and $ByteMask(k)$ returns 
a bit mask for k bytes. 

(maybe to future work) Storing bytes in this fashion may have benefits when reading subsequent (fast next() and previous()?) elements. With some modifications, SIMD instructions can possibly be applied.

\begin{figure}[ht]
\begin{algorithmic}
\State $i \gets $ wanted index
\State $A \gets $ block byte array
\State $B \gets $ continuation bit array
\State $begin \gets \Call{Select}{index}$
\State $len \gets \Call{CalculateLength}{begin}$
\State $mask \gets \Call{ByteMask}{len}$
\State $number \gets A[begin]$ \Comment{read a larger number from the byte array}
\State $number \gets number \mathbin{\&} mask$ 


\end{algorithmic}
\caption{Pseudo code of DAC with select with block length 8} \label{select_pseudo}
\end{figure}

TODO: try bitvector SD performance + size (select)

TODO: look into select implementation, may need optimizing

\chapter{Experimental results}

The following experiments are run on <at the moment my work Lenovo Thinkpad T480s, TODO run on a server and write specs here>. The algorithms 
were implemented with C++ (TODO: list compile tags and whatnot here) using data structures and functions from \citep{gbmp2014sea}. 



Five different kinds of VB decoding algorithms were used. Bris-implementations are from \citep{Bri09}. 
\begin{itemize}
  \item Bris4v5 - Bris implementation with block size 4, using rank support v5
  \item Bris8 - Bris implementation with block size 8, using rank support v
  \item Bris8v5 - Bris implementation with block size 8, using rank support v5
  \item Our4 - Our implementation with block size 4, using select support mcl
  \item Our8 - Our implementation with block size 8, using select support mcl
\end{itemize}

Eight kinds of datasets were used. For each number in the dataset, $p$ was randomly selected from $P$ and then number was randomly selected from
range [0,$2^p$].

\begin{itemize}
  \item all5M - 5 million numbers, $P$ = {7,8,15,16,23,24,30,31}
  \item all50M - 50 million numbers, $P$ = {7,8,15,16,23,24,30,31}
  \item byte5M - 5 million numbers, $P$ = {7,7,7,8,8,8,16,31}
  \item byte50M - 50 million numbers, $P$ = {7,7,7,8,8,8,16,31}
  \item small5M - 5 million numbers, $P$ = {3,4,5,6,7,8,16,31}
  \item small50M - 50 million numbers, $P$ = {3,4,5,6,7,8,16,31}
  \item vsmall5M - 5 million numbers, $P$ = {2,2,3,3,3,4,4,15}
  \item vsmall50M - 50 million numbers, $P$ = {2,2,3,3,3,4,4,15}
\end{itemize}

All algorithms save the data to memory and randomize one million index numbers to an array. The time taken is calculated from looping through the 
index array and VB decoding the number in the index. The times shown in Table~\ref{table:results1} are an average of 100 such runs.

\begin{table}
\centering
\caption{Results in milliseconds, smaller is better.\label{table:results1}}
\begin{tabular}{l||c c c c c c c c} 
Experiment & all5M & all50M & byte5M & byte50M & small5M & small50M & vsmall5M & vsmall50M\\ 
\hline \hline 
Bris4v5 & 348.41 & 768.83 & 345.16 & 772.84 & 344.54 & 768.45 & 346.86 & 771.61 \\
Our4    & 127.92 & 263.22 & 127.44 & 263.5  & 127.9  & 262.67 & 127.88 & 262.58 \\
Bris8   & \textbf{97.75}  & 308.17 & \textbf{98.47}  & 309.6  & \textbf{98.14}  & 307.81 & \textbf{97.99}  & 308.09 \\
Bris8v5 & 127.83 & 287.46 & 127.7  & 288.63 & 128.14 & 291.32 & 127.99 & 287.84 \\
Our8    & 103.85 & \textbf{230.16} & 103.92 & \textbf{230.14} & 103.86 & \textbf{230.02} & 103.95 & \textbf{230.6} \\

\hline
%
\end{tabular}
\end{table}



 - Run with 32 bit values, see what happens (some operations may be 32b?)
 - do cachegrind
 - 
 - use array instead of vector<uint8 t> for bris?
- comparison to basic implementation + Bri09
  - compare with b=2,4,8

 - do we need to support search()?
 - SIMD?

 - $rank$ vs rank? should formulas have a specific style?

\chapter{Future work}
 - something to improve / research?

\chapter{Conclusions\label{chapter:conclusions}}

It is good to conclude with a summary of findings. You can also use separate chapter for discussion and future work. These details you can negotiate with your supervisor.
